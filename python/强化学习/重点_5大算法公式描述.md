### 1.比较
\		a1	a2	a3	a4	...	am
s1
s2
...
sn
## 1&2. QL与SarSa
![7646211b2931123708d524f49d395363.png](file:///C:/Users/%E8%81%94%E6%83%B3/.config/joplin-desktop/resources/95e1d3cb031d4f749e71bd6915aa6554.png?t=1656558514553)
## 3\. DQL
![ae26b785b4506c481ad6833f34ca656b.png](../../_resources/ae26b785b4506c481ad6833f34ca656b.png)
**利用Q估计和Q现实的差值，反向传播，更快收敛**
*通常搭建两个神经网络，target\_net 用于预测q\_target 值，它不会及时更新参数，这组参数被固定一段时间，然后被eval_net 的新参数替换。***（Q(s',a';θ)为s' adopt a'获得的动作奖励）**
![e48de5ec37cc661e464f38e61423e02b.png](file:///C:/Users/%E8%81%94%E6%83%B3/.config/joplin-desktop/resources/6c54bb2ee5d94e9ca4ab502fc4a8ddea.png?t=1656558547354)
## [4\. Policy learning](http://t.csdn.cn/bb782)
它能在一个连续区间内挑选动作, 而基于值的, 比如 Q-learning, 它如果在无穷多的动作中计算价值, 从而选择行为, 这, 它可吃不消。
**先依照概率选择行为，再增加好行为的概率**!
//或者说先任意行动，然后选择为动作的拟合
而Q-learning，为选择Q最高的动作

![48327b791a9e3e894b533d59c7fac630.png](../../_resources/48327b791a9e3e894b533d59c7fac630.png)
**表明R(t)是θ的函数**

![8db2898aa3b8470d0edfcd3d721faf3d.png](file:///C:/Users/%E8%81%94%E6%83%B3/.config/joplin-desktop/resources/f575eeb8c22544439f8c7f4d577bf155.png?t=1656560428874)
![7732f03b17580bd27f34bcb83b51db94.png](file:///C:/Users/%E8%81%94%E6%83%B3/.config/joplin-desktop/resources/a24a4ed1eebf4f79ba5462ecc473c7d2.png?t=1656560440088)
[5\. Actor-Critic=policy grdient+value-based](https://blog.csdn.net/qq_30615903/article/details/80774384)

## **[2.dqn_ppo总结](http://t.csdn.cn/mLH6Q)**